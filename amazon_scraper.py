"""
K-Beauty Amazon Review Scraper
- Target: Amazon US 'Korean Skincare' ê²€ìƒ‰ ê²°ê³¼
- ìƒìœ„ 500ê°œ ì œí’ˆì˜ ìµœê·¼ ë¦¬ë·° 20ê°œì”© ìˆ˜ì§‘

ì‚¬ìš©ë²•:
1. ë¨¼ì € Chromeì„ ë””ë²„ê·¸ ëª¨ë“œë¡œ ì‹¤í–‰:
   /Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome --remote-debugging-port=9222

2. ì—´ë¦° Chromeì—ì„œ Amazon ë¡œê·¸ì¸

3. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰:
   python3 amazon_scraper.py
"""

import csv
import time
import random
import ssl
import os
from datetime import datetime
from typing import Optional

# macOS SSL ì¸ì¦ì„œ ë¬¸ì œ ìš°íšŒ
ssl._create_default_https_context = ssl._create_unverified_context
os.environ['WDM_SSL_VERIFY'] = '0'

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import (
    NoSuchElementException,
    TimeoutException,
    StaleElementReferenceException
)
from bs4 import BeautifulSoup


class AmazonKBeautyScraper:
    """Amazon K-Beauty ë¦¬ë·° í¬ë¡¤ëŸ¬"""
    
    BASE_URL = "https://www.amazon.com"
    SEARCH_KEYWORD = "Korean Skincare"
    MAX_PRODUCTS = 500  # ìƒìœ„ 500ê°œ ì œí’ˆ
    MAX_REVIEWS_PER_PRODUCT = 20  # ì œí’ˆë‹¹ ë¦¬ë·° ìˆ˜
    DEBUG_PORT = 9222
    
    def __init__(self):
        self.driver = None
        self.reviews_data = []
        self.products_data = []  # ì œí’ˆ ë©”íƒ€ë°ì´í„° ì €ì¥
    
    def _random_delay(self, min_sec: float = 2, max_sec: float = 5):
        """ì‚¬ëŒì²˜ëŸ¼ ë³´ì´ê²Œ ëœë¤ ë”œë ˆì´"""
        time.sleep(random.uniform(min_sec, max_sec))
    
    def _init_driver(self):
        """ê¸°ì¡´ Chrome ì°½ì— ì—°ê²°"""
        options = Options()
        options.add_experimental_option("debuggerAddress", f"127.0.0.1:{self.DEBUG_PORT}")
        
        try:
            self.driver = webdriver.Chrome(options=options)
            print(f"[INFO] Connected to existing Chrome (port {self.DEBUG_PORT})")
            print(f"[INFO] Current URL: {self.driver.current_url}")
        except Exception as e:
            print(f"[ERROR] Chrome ì—°ê²° ì‹¤íŒ¨: {e}")
            print("\n" + "="*60)
            print("ë¨¼ì € Chromeì„ ë””ë²„ê·¸ ëª¨ë“œë¡œ ì‹¤í–‰í•˜ì„¸ìš”:")
            print("="*60)
            print('/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --remote-debugging-port=9222')
            print("="*60)
            raise
    
    def _close_driver(self):
        """ë“œë¼ì´ë²„ ì—°ê²° í•´ì œ (ë¸Œë¼ìš°ì €ëŠ” ë‹«ì§€ ì•ŠìŒ)"""
        if self.driver:
            # quit() ëŒ€ì‹  ì—°ê²°ë§Œ í•´ì œ - ë¸Œë¼ìš°ì €ëŠ” ê³„ì† ì—´ë ¤ìˆìŒ
            print("[INFO] Chrome driver disconnected (browser stays open)")

    def search_products(self) -> list[str]:
        """Korean Skincare ê²€ìƒ‰ í›„ ìƒìœ„ ì œí’ˆ URL ìˆ˜ì§‘ (í˜ì´ì§€ë„¤ì´ì…˜ í¬í•¨)"""
        product_urls = []
        page = 1
        no_new_products_count = 0  # ì—°ì†ìœ¼ë¡œ ìƒˆ ì œí’ˆì´ ì—†ëŠ” í˜ì´ì§€ ìˆ˜
        
        try:
            while len(product_urls) < self.MAX_PRODUCTS:
                # ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™ (í˜ì´ì§€ë„¤ì´ì…˜)
                search_url = f"{self.BASE_URL}/s?k={self.SEARCH_KEYWORD.replace(' ', '+')}&page={page}"
                print(f"\n[INFO] Navigating to page {page}: {search_url}")
                self.driver.get(search_url)
                self._random_delay(4, 7)
                
                # ì²« í˜ì´ì§€ë§Œ ìŠ¤í¬ë¦°ìƒ·
                if page == 1:
                    self.driver.save_screenshot("debug_search_page.png")
                    print("[DEBUG] Screenshot saved: debug_search_page.png")
                
                print(f"[DEBUG] Page title: {self.driver.title}")
                
                # í˜ì´ì§€ ì†ŒìŠ¤
                page_source = self.driver.page_source
                
                # í˜ì´ì§€ ë ì²´í¬ (Amazonì˜ "No more results" ë©”ì‹œì§€)
                if "No results for" in page_source or "did not match any products" in page_source:
                    print(f"[INFO] Reached end of search results at page {page}")
                    break
                
                # CAPTCHA ì²´í¬
                soup_check = BeautifulSoup(page_source, "html.parser")
                captcha_form = soup_check.find("form", {"action": lambda x: x and "validateCaptcha" in x if x else False})
                has_captcha = captcha_form is not None
                
                if has_captcha:
                    print("\n" + "="*60)
                    print("[CAPTCHA] CAPTCHAê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    print("ë¸Œë¼ìš°ì €ì—ì„œ CAPTCHAë¥¼ í’€ì–´ì£¼ì„¸ìš”.")
                    print("ì™„ë£Œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”...")
                    print("="*60)
                    input()
                    page_source = self.driver.page_source
                    print("[INFO] CAPTCHA í•´ê²° í›„ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                
                # ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„
                selectors = [
                    "[data-component-type='s-search-result']",
                    "div.s-result-item[data-asin]",
                    ".s-main-slot .s-result-item",
                    "[data-cel-widget^='search_result']",
                    "div[data-asin]"
                ]
                
                soup = BeautifulSoup(page_source, "html.parser")
                product_cards = []
                
                for selector in selectors:
                    try:
                        product_cards = soup.select(selector)
                        product_cards = [c for c in product_cards if c.get("data-asin")]
                        if product_cards:
                            print(f"[INFO] Found {len(product_cards)} products on page {page}")
                            break
                    except Exception as e:
                        continue
                
                if not product_cards:
                    print(f"[WARN] No products found on page {page}")
                    no_new_products_count += 1
                    # ì—°ì† 3í˜ì´ì§€ ì œí’ˆ ì—†ìœ¼ë©´ ì¢…ë£Œ
                    if no_new_products_count >= 3:
                        print("[INFO] No products found for 3 consecutive pages, stopping")
                        break
                    page += 1
                    continue
                
                # ì œí’ˆ URL ì¶”ì¶œ
                page_products = 0
                for card in product_cards:
                    if len(product_urls) >= self.MAX_PRODUCTS:
                        break
                    try:
                        link_tag = (
                            card.select_one("h2 a.a-link-normal") or
                            card.select_one("a.a-link-normal.s-no-outline") or
                            card.select_one("a.a-link-normal[href*='/dp/']") or
                            card.select_one("a[href*='/dp/']") or
                            card.select_one("a[href]")
                        )
                        if link_tag and link_tag.get("href"):
                            href = link_tag["href"]
                            if "/dp/" not in href and "/gp/" not in href:
                                continue
                            full_url = href if href.startswith("http") else f"{self.BASE_URL}{href}"
                            if full_url not in product_urls:
                                product_urls.append(full_url)
                                page_products += 1
                    except Exception as e:
                        continue
                
                print(f"[INFO] Collected {page_products} products from page {page} (Total: {len(product_urls)})")
                
                # ìƒˆ ì œí’ˆì´ ì—†ìœ¼ë©´ ì¹´ìš´íŠ¸ ì¦ê°€
                if page_products == 0:
                    no_new_products_count += 1
                    if no_new_products_count >= 3:
                        print("[INFO] No new products for 3 consecutive pages, stopping")
                        break
                else:
                    no_new_products_count = 0  # ë¦¬ì…‹
                
                # ëª©í‘œ ë‹¬ì„± ì²´í¬
                if len(product_urls) >= self.MAX_PRODUCTS:
                    print(f"[INFO] Reached target of {self.MAX_PRODUCTS} products")
                    break
                
                # ë‹¤ìŒ í˜ì´ì§€ë¡œ
                page += 1
                
                # ì•ˆì „ì¥ì¹˜: ìµœëŒ€ 100í˜ì´ì§€ê¹Œì§€ë§Œ
                if page > 100:
                    print("[WARN] Reached maximum page limit (100), stopping")
                    break
                
                self._random_delay(3, 5)
            
            print(f"\n[INFO] Total {len(product_urls)} products collected")
            
        except Exception as e:
            print(f"[ERROR] Search failed: {e}")
            import traceback
            traceback.print_exc()
        
        return product_urls
    
    def _get_product_name(self) -> str:
        """ì œí’ˆëª… ì¶”ì¶œ"""
        try:
            title_elem = self.driver.find_element(By.ID, "productTitle")
            return title_elem.text.strip()
        except NoSuchElementException:
            return "Unknown Product"
    
    def _navigate_to_reviews(self) -> bool:
        """ë¦¬ë·° í˜ì´ì§€ë¡œ ì´ë™"""
        try:
            # 'See all reviews' ë§í¬ ì°¾ê¸°
            review_link = WebDriverWait(self.driver, 10).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "[data-hook='see-all-reviews-link-foot']"))
            )
            review_link.click()
            self._random_delay(2, 4)
            return True
        except (TimeoutException, NoSuchElementException):
            # ëŒ€ì²´ ë°©ë²•: URL ì§ì ‘ ìˆ˜ì •
            try:
                current_url = self.driver.current_url
                if "/dp/" in current_url:
                    asin = current_url.split("/dp/")[1].split("/")[0].split("?")[0]
                    reviews_url = f"{self.BASE_URL}/product-reviews/{asin}"
                    self.driver.get(reviews_url)
                    self._random_delay(2, 4)
                    return True
            except Exception as e:
                print(f"[WARN] Could not navigate to reviews: {e}")
        return False

    def _parse_reviews(self, product_name: str) -> list[dict]:
        """í˜„ì¬ í˜ì´ì§€ì—ì„œ ë¦¬ë·° íŒŒì‹±"""
        reviews = []
        
        try:
            soup = BeautifulSoup(self.driver.page_source, "html.parser")
            review_cards = soup.select("[data-hook='review']")
            
            for card in review_cards:
                if len(reviews) >= self.MAX_REVIEWS_PER_PRODUCT:
                    break
                
                try:
                    review = self._extract_review_data(card, product_name)
                    if review:
                        reviews.append(review)
                        print(f"  [OK] Review #{len(reviews)} collected")
                except Exception as e:
                    print(f"  [WARN] Failed to parse review: {e}")
                    continue
                    
        except Exception as e:
            print(f"[ERROR] Review parsing failed: {e}")
        
        return reviews
    
    def _extract_review_data(self, card, product_name: str) -> Optional[dict]:
        """ê°œë³„ ë¦¬ë·° ë°ì´í„° ì¶”ì¶œ"""
        import re
        from datetime import datetime
        
        # ë¦¬ë·° ë³¸ë¬¸
        review_body = card.select_one("[data-hook='review-body']")
        review_text = review_body.get_text(strip=True) if review_body else ""
        
        if not review_text:
            return None
        
        # ë³„ì  ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
        rating = 0.0
        
        # ë°©ë²• 1: i íƒœê·¸ì˜ classì—ì„œ ì¶”ì¶œ (ê°€ì¥ ì •í™•)
        rating_elem = card.select_one("i[data-hook='review-star-rating'], i[data-hook='cmps-review-star-rating']")
        if rating_elem:
            # class="a-icon a-icon-star a-star-4" í˜•ì‹ì—ì„œ ìˆ«ì ì¶”ì¶œ
            class_str = rating_elem.get("class", [])
            if isinstance(class_str, list):
                class_str = " ".join(class_str)
            
            # a-star-4, a-star-4-5 ë“±ì˜ íŒ¨í„´ ì°¾ê¸°
            match = re.search(r'a-star-(\d+(?:-\d+)?)', class_str)
            if match:
                star_value = match.group(1)
                if "-" in star_value:
                    # "4-5" -> 4.5
                    parts = star_value.split("-")
                    rating = float(f"{parts[0]}.{parts[1]}")
                else:
                    # "4" -> 4.0
                    rating = float(star_value)
        
        # ë°©ë²• 2: span í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
        if rating == 0.0:
            rating_elem = card.select_one("[data-hook='review-star-rating'] span, [data-hook='cmps-review-star-rating'] span")
            if rating_elem:
                rating_text = rating_elem.get_text(strip=True)
                # "4.0 out of 5 stars" í˜•ì‹ì—ì„œ ì²« ë²ˆì§¸ ìˆ«ì ì¶”ì¶œ
                match = re.search(r'^(\d+\.?\d*)', rating_text)
                if match:
                    rating = float(match.group(1))
        
        # ë°©ë²• 3: ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
        if rating == 0.0:
            rating_elem = card.select_one("[data-hook='review-star-rating'], [data-hook='cmps-review-star-rating']")
            if rating_elem:
                rating_text = rating_elem.get_text(strip=True)
                match = re.search(r'^(\d+\.?\d*)', rating_text)
                if match:
                    rating = float(match.group(1))
        
        # ë‚ ì§œ ì¶”ì¶œ ë° ë³€í™˜
        date_elem = card.select_one("[data-hook='review-date']")
        review_date_raw = date_elem.get_text(strip=True) if date_elem else ""
        
        # ë‚ ì§œë¥¼ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        review_date = ""
        if review_date_raw:
            try:
                # "Reviewed in the United States on December 20, 2025" í˜•ì‹
                # ë˜ëŠ” "ë¯¸êµ­ì—ì„œ 2025ë…„ 11ì›” 24ì¼ì— ê²€í† ë¨" í˜•ì‹
                
                # ì˜ì–´ ë‚ ì§œ íŒŒì‹±
                if " on " in review_date_raw:
                    date_part = review_date_raw.split(" on ")[-1]
                    parsed_date = datetime.strptime(date_part, "%B %d, %Y")
                    review_date = parsed_date.strftime("%Y-%m-%d")
                # í•œêµ­ì–´ ë‚ ì§œ íŒŒì‹±
                elif "ë…„" in review_date_raw and "ì›”" in review_date_raw:
                    # "2025ë…„ 11ì›” 24ì¼" ì¶”ì¶œ
                    match = re.search(r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼', review_date_raw)
                    if match:
                        year, month, day = match.groups()
                        review_date = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                else:
                    review_date = review_date_raw  # íŒŒì‹± ì‹¤íŒ¨ì‹œ ì›ë³¸ ìœ ì§€
            except Exception as e:
                print(f"  [DEBUG] Date parsing failed: {review_date_raw} - {e}")
                review_date = review_date_raw
        
        # "ìœ ìš©í•¨" íˆ¬í‘œ ìˆ˜ ì¶”ì¶œ
        helpful_count = 0
        helpful_elem = card.select_one("[data-hook='helpful-vote-statement']")
        if helpful_elem:
            helpful_text = helpful_elem.get_text(strip=True)
            # "123 people found this helpful" í˜•ì‹
            match = re.search(r'([\d,]+)\s+people', helpful_text)
            if match:
                helpful_count = int(match.group(1).replace(",", ""))
            # "One person found this helpful" í˜•ì‹
            elif "One person" in helpful_text or "1 person" in helpful_text:
                helpful_count = 1
        
        # ê²€ì¦ëœ êµ¬ë§¤ ì—¬ë¶€
        verified_purchase = False
        verified_elem = card.select_one("[data-hook='avp-badge']")
        if verified_elem:
            verified_purchase = True
        
        return {
            "product_name": product_name,
            "review_text": review_text,
            "rating": rating,
            "date": review_date,
            "helpful_count": helpful_count,
            "verified_purchase": verified_purchase,
            "source": "amazon"
        }
    
    def _load_more_reviews(self) -> bool:
        """ë‹¤ìŒ í˜ì´ì§€ ë¦¬ë·° ë¡œë“œ"""
        try:
            next_btn = self.driver.find_element(By.CSS_SELECTOR, "li.a-last a")
            if next_btn:
                next_btn.click()
                self._random_delay(2, 4)
                return True
        except NoSuchElementException:
            pass
        return False

    def scrape_product_reviews(self, product_url: str) -> list[dict]:
        """ë‹¨ì¼ ì œí’ˆì˜ ë¦¬ë·° ìˆ˜ì§‘ + ì œí’ˆ ë©”íƒ€ë°ì´í„°"""
        reviews = []
        
        try:
            print(f"\n[INFO] Scraping product: {product_url[:60]}...")
            self.driver.get(product_url)
            self._random_delay(3, 5)
            
            # ì œí’ˆ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
            product_metadata = self._extract_product_metadata()
            self.products_data.append(product_metadata)
            
            product_name = product_metadata.get("product_name", "Unknown Product")
            print(f"[INFO] Product: {product_name[:50]}...")
            print(f"[INFO] Price: {product_metadata.get('price', 'N/A')}")
            print(f"[INFO] Rating: {product_metadata.get('avg_rating', 'N/A')} ({product_metadata.get('review_count', 0)} reviews)")
            
            # ë¦¬ë·° í˜ì´ì§€ë¡œ ì´ë™
            if not self._navigate_to_reviews():
                print("[WARN] Could not navigate to reviews page")
                return reviews
            
            # ë¦¬ë·° ìˆ˜ì§‘ (í˜ì´ì§€ë„¤ì´ì…˜ í¬í•¨)
            page = 1
            while len(reviews) < self.MAX_REVIEWS_PER_PRODUCT:
                print(f"[INFO] Parsing reviews page {page}...")
                
                page_reviews = self._parse_reviews(product_name)
                if not page_reviews:
                    break
                
                reviews.extend(page_reviews)
                
                if len(reviews) >= self.MAX_REVIEWS_PER_PRODUCT:
                    break
                
                if not self._load_more_reviews():
                    break
                
                page += 1
                self._random_delay(2, 4)
            
            print(f"[INFO] Collected {len(reviews)} reviews for this product")
            
        except Exception as e:
            print(f"[ERROR] Failed to scrape product: {e}")
        
        return reviews[:self.MAX_REVIEWS_PER_PRODUCT]
    
    def _extract_product_metadata(self) -> dict:
        """ì œí’ˆ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ê°€ê²©, í‰ì , ë¦¬ë·° ìˆ˜, íŒë§¤ëŸ‰ ë“±)"""
        metadata = {
            "product_name": "Unknown Product",
            "price": None,
            "avg_rating": None,
            "review_count": 0,
            "brand": None,
            "category": None,
            "bought_last_month": 0,  # ì§€ë‚œë‹¬ êµ¬ë§¤ ìˆ˜
            "url": self.driver.current_url
        }
        
        try:
            soup = BeautifulSoup(self.driver.page_source, "html.parser")
            
            # ì œí’ˆëª…
            title_elem = soup.select_one("#productTitle")
            if title_elem:
                metadata["product_name"] = title_elem.get_text(strip=True)
            
            # ê°€ê²©
            price_elem = (
                soup.select_one(".a-price .a-offscreen") or
                soup.select_one("#priceblock_ourprice") or
                soup.select_one("#priceblock_dealprice")
            )
            if price_elem:
                metadata["price"] = price_elem.get_text(strip=True)
            
            # í‰ê·  ë³„ì 
            rating_elem = soup.select_one("[data-hook='rating-out-of-text']")
            if rating_elem:
                import re
                rating_text = rating_elem.get_text(strip=True)
                match = re.search(r'(\d+\.?\d*)', rating_text)
                if match:
                    metadata["avg_rating"] = float(match.group(1))
            
            # ë¦¬ë·° ìˆ˜
            review_count_elem = soup.select_one("#acrCustomerReviewText")
            if review_count_elem:
                import re
                count_text = review_count_elem.get_text(strip=True)
                match = re.search(r'([\d,]+)', count_text)
                if match:
                    metadata["review_count"] = int(match.group(1).replace(",", ""))
            
            # ë¸Œëœë“œ
            brand_elem = soup.select_one("#bylineInfo")
            if brand_elem:
                metadata["brand"] = brand_elem.get_text(strip=True).replace("Visit the ", "").replace(" Store", "")
            
            # ì§€ë‚œë‹¬ êµ¬ë§¤ ìˆ˜ (ì˜ˆ: "10K+ bought in past month")
            # ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„
            bought_selectors = [
                "#social-proofing-faceout-title-tk_bought",
                "[id*='social-proofing']",
                ".social-proofing-widget-text"
            ]
            
            bought_elem = None
            for selector in bought_selectors:
                bought_elem = soup.select_one(selector)
                if bought_elem:
                    break
            
            # í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì°¾ê¸°
            if not bought_elem:
                bought_elem = soup.find(string=re.compile(r'bought in past month', re.IGNORECASE))
            
            if bought_elem:
                bought_text = bought_elem if isinstance(bought_elem, str) else bought_elem.get_text(strip=True)
                print(f"[DEBUG] Found bought text: {bought_text}")
                
                # "10K+ bought" ë˜ëŠ” "500+ bought" í˜•ì‹
                match = re.search(r'([\d.]+)([KM])?\+?\s*bought', bought_text, re.IGNORECASE)
                if match:
                    number = float(match.group(1))
                    multiplier = match.group(2)
                    if multiplier == 'K':
                        number *= 1000
                    elif multiplier == 'M':
                        number *= 1000000
                    metadata["bought_last_month"] = int(number)
                    print(f"[DEBUG] Extracted bought_last_month: {metadata['bought_last_month']}")
                else:
                    # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš°
                    match = re.search(r'([\d,]+)', bought_text)
                    if match:
                        metadata["bought_last_month"] = int(match.group(1).replace(",", ""))
                        print(f"[DEBUG] Extracted bought_last_month: {metadata['bought_last_month']}")
            else:
                print("[DEBUG] No 'bought in past month' data found")
            
        except Exception as e:
            print(f"[DEBUG] Metadata extraction failed: {e}")
        
        return metadata
    
    def run(self):
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
        print("=" * 60)
        print("K-Beauty Amazon Review Scraper")
        print(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)
        print("\nğŸ’¡ Tip: Ctrl+Cë¥¼ ëˆŒëŸ¬ ì–¸ì œë“ ì§€ ì¤‘ë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§€ê¸ˆê¹Œì§€ ìˆ˜ì§‘í•œ ë°ì´í„°ëŠ” ìë™ ì €ì¥ë©ë‹ˆë‹¤.\n")
        
        try:
            self._init_driver()
            
            # 1. ì œí’ˆ ê²€ìƒ‰
            product_urls = self.search_products()
            
            if not product_urls:
                print("[ERROR] No products found")
                return
            
            # 2. ê° ì œí’ˆ ë¦¬ë·° ìˆ˜ì§‘
            for idx, url in enumerate(product_urls, 1):
                print(f"\n{'='*40}")
                print(f"Processing product {idx}/{len(product_urls)}")
                print(f"{'='*40}")
                
                reviews = self.scrape_product_reviews(url)
                self.reviews_data.extend(reviews)
                
                # ì œí’ˆ ê°„ ë”œë ˆì´
                if idx < len(product_urls):
                    self._random_delay(3, 6)
            
            # 3. ê²°ê³¼ ì €ì¥
            self._save_results()
            
        except KeyboardInterrupt:
            print("\n\n" + "="*60)
            print("âš ï¸  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤ (Ctrl+C)")
            print("="*60)
            print(f"[INFO] ì§€ê¸ˆê¹Œì§€ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤...")
            print(f"[INFO] ìˆ˜ì§‘ëœ ì œí’ˆ: {len(self.products_data)}ê°œ")
            print(f"[INFO] ìˆ˜ì§‘ëœ ë¦¬ë·°: {len(self.reviews_data)}ê°œ")
            self._save_results()
            
        except Exception as e:
            print(f"\n[FATAL] Scraper crashed: {e}")
            print(f"[INFO] ì§€ê¸ˆê¹Œì§€ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤...")
            if self.reviews_data or self.products_data:
                self._save_results()
                
        finally:
            self._close_driver()
    
    def _save_results(self):
        """ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥"""
        import csv
        import os
        
        # results í´ë” ìƒì„±
        os.makedirs("results", exist_ok=True)
        
        # ë¦¬ë·° CSV ì €ì¥
        reviews_file = "results/amazon_reviews.csv"
        
        if self.reviews_data:
            cleaned_data = []
            for review in self.reviews_data:
                cleaned_review = {
                    "product_name": review.get("product_name", "").replace("\n", " ").replace("\r", " "),
                    "review_text": review.get("review_text", "").replace("\n", " ").replace("\r", " "),
                    "rating": review.get("rating", 0),
                    "date": review.get("date", ""),
                    "helpful_count": review.get("helpful_count", 0),
                    "verified_purchase": review.get("verified_purchase", False),
                    "source": review.get("source", "")
                }
                cleaned_data.append(cleaned_review)
            
            with open(reviews_file, "w", encoding="utf-8-sig", newline="") as f:
                writer = csv.DictWriter(f, fieldnames=["product_name", "review_text", "rating", "date", "helpful_count", "verified_purchase", "source"])
                writer.writeheader()
                writer.writerows(cleaned_data)
            
            print(f"\n[SUCCESS] Saved {len(self.reviews_data)} reviews to {reviews_file}")
        
        # ì œí’ˆ ë©”íƒ€ë°ì´í„° CSV ì €ì¥
        products_file = "results/amazon_products.csv"
        
        if self.products_data:
            with open(products_file, "w", encoding="utf-8-sig", newline="") as f:
                fieldnames = ["product_name", "brand", "price", "avg_rating", "review_count", "bought_last_month", "url"]
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(self.products_data)
            
            print(f"[SUCCESS] Saved {len(self.products_data)} products to {products_file}")


if __name__ == "__main__":
    scraper = AmazonKBeautyScraper()
    scraper.run()
