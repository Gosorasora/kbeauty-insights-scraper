"""
Azure AI Search Migration Tool v2.0
===================================

CSV ë¦¬ë·° ë°ì´í„°ë¥¼ Azure AI Search Vector ì¸ë±ìŠ¤ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜

ì£¼ìš” ê¸°ëŠ¥:
- CSV ë°ì´í„° ìë™ ë¡œë“œ ë° ì •ì œ
- Azure OpenAIë¡œ ì„ë² ë”© ìƒì„± (text-embedding-ada-002)
- Vector Search ì¸ë±ìŠ¤ ìƒì„±
- ë°°ì¹˜ ì—…ë¡œë“œë¡œ ì„±ëŠ¥ ìµœì í™”
- ì—…ë¡œë“œ ì§„í–‰ë¥  í‘œì‹œ

ì‚¬ìš©ë²•:
1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (.env íŒŒì¼)
2. CSV íŒŒì¼ì„ results/ í´ë”ì— ë°°ì¹˜
3. python migrate_to_azure_search.py ì‹¤í–‰

ì§€ì› íŒŒì¼: amazon_reviews.csv, kbeauty_reviews.csv
"""

import os
import pandas as pd
import json
from typing import List, Dict, Any
from datetime import datetime

from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import (
    SearchIndex,
    SearchField,
    SearchFieldDataType,
    SimpleField,
    SearchableField,
    VectorSearch,
    HnswAlgorithmConfiguration,
    VectorSearchProfile,
    SemanticConfiguration,
    SemanticSearch,
    SemanticField,
    SemanticPrioritizedFields
)
from azure.core.credentials import AzureKeyCredential
from openai import AzureOpenAI
from tqdm import tqdm
import time

class AzureSearchMigrator:
    """Azure AI Search ë§ˆì´ê·¸ë ˆì´ì…˜ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        
        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        self.search_endpoint = os.getenv("AZURE_SEARCH_ENDPOINT")
        self.search_key = os.getenv("AZURE_SEARCH_API_KEY")
        self.openai_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
        self.openai_key = os.getenv("AZURE_OPENAI_API_KEY")
        self.index_name = os.getenv("SEARCH_INDEX_NAME", "amor-party-reviews")
        
        if not all([self.search_endpoint, self.search_key, self.openai_endpoint, self.openai_key]):
            raise ValueError("í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.credential = AzureKeyCredential(self.search_key)
        self.index_client = SearchIndexClient(
            endpoint=self.search_endpoint,
            credential=self.credential
        )
        self.search_client = SearchClient(
            endpoint=self.search_endpoint,
            index_name=self.index_name,
            credential=self.credential
        )
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸
        self.openai_client = AzureOpenAI(
            api_key=self.openai_key,
            api_version="2024-02-15-preview",
            azure_endpoint=self.openai_endpoint
        )
        
        print("âœ“ Azure AI Search ë§ˆì´ê·¸ë ˆì´í„° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def create_search_index(self) -> None:
        """ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„±"""
        
        print(f"ğŸ”§ ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„± ì¤‘: {self.index_name}")
        
        # Vector Search ì„¤ì •
        vector_search = VectorSearch(
            algorithms=[
                HnswAlgorithmConfiguration(
                    name="myHnsw",
                    parameters={
                        "m": 4,
                        "efConstruction": 400,
                        "efSearch": 500,
                        "metric": "cosine"
                    }
                )
            ],
            profiles=[
                VectorSearchProfile(
                    name="myHnswProfile",
                    algorithm_configuration_name="myHnsw"
                )
            ]
        )
        
        # Semantic Search ì„¤ì •
        semantic_config = SemanticConfiguration(
            name="my-semantic-config",
            prioritized_fields=SemanticPrioritizedFields(
                content_fields=[SemanticField(field_name="review_text")],
                keywords_fields=[SemanticField(field_name="product_name")]
            )
        )
        
        semantic_search = SemanticSearch(configurations=[semantic_config])
        
        # í•„ë“œ ì •ì˜
        fields = [
            SimpleField(name="id", type=SearchFieldDataType.String, key=True),
            SearchableField(name="product_name", type=SearchFieldDataType.String),
            SearchableField(name="review_text", type=SearchFieldDataType.String),
            SimpleField(name="rating", type=SearchFieldDataType.Double),
            SimpleField(name="date", type=SearchFieldDataType.String),
            SimpleField(name="helpful_count", type=SearchFieldDataType.Int32),
            SimpleField(name="verified_purchase", type=SearchFieldDataType.Boolean),
            SearchField(
                name="embedding",
                type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                searchable=True,
                vector_search_dimensions=1536,
                vector_search_profile_name="myHnswProfile"
            )
        ]
        
        # ì¸ë±ìŠ¤ ìƒì„±
        index = SearchIndex(
            name=self.index_name,
            fields=fields,
            vector_search=vector_search,
            semantic_search=semantic_search
        )
        
        try:
            result = self.index_client.create_or_update_index(index)
            print(f"âœ“ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {result.name}")
        except Exception as e:
            print(f"âŒ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def get_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        try:
            response = self.openai_client.embeddings.create(
                input=text,
                model="text-embedding-ada-002"
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return [0.0] * 1536  # ê¸°ë³¸ê°’
    
    def load_csv_data(self, csv_path: str) -> pd.DataFrame:
        """CSV ë°ì´í„° ë¡œë“œ"""
        
        print(f"ğŸ“‚ CSV ë°ì´í„° ë¡œë“œ ì¤‘: {csv_path}")
        
        try:
            df = pd.read_csv(csv_path)
            print(f"âœ“ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df):,}ê°œ ë¦¬ë·°")
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            required_columns = ['product_name', 'review_text', 'rating', 'date']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
            
            # ë°ì´í„° ì •ì œ
            df = df.dropna(subset=['review_text'])
            df['helpful_count'] = df.get('helpful_count', 0).fillna(0).astype(int)
            df['verified_purchase'] = df.get('verified_purchase', True).fillna(True).astype(bool)
            
            print(f"âœ“ ë°ì´í„° ì •ì œ ì™„ë£Œ: {len(df):,}ê°œ ë¦¬ë·°")
            return df
            
        except Exception as e:
            print(f"âŒ CSV ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def prepare_documents(self, df: pd.DataFrame, batch_size: int = 100) -> List[List[Dict[str, Any]]]:
        """ë¬¸ì„œ ì¤€ë¹„ (ë°°ì¹˜ ë‹¨ìœ„)"""
        
        print(f"ğŸ“ ë¬¸ì„œ ì¤€ë¹„ ì¤‘ (ë°°ì¹˜ í¬ê¸°: {batch_size})")
        
        documents = []
        batches = []
        
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="ì„ë² ë”© ìƒì„±"):
            # ë¦¬ë·° í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
            review_text = str(row['review_text'])[:2000]  # ê¸¸ì´ ì œí•œ
            embedding = self.get_embedding(review_text)
            
            # ë¬¸ì„œ ìƒì„±
            doc = {
                "id": f"review_{idx}",
                "product_name": str(row['product_name'])[:500],
                "review_text": review_text,
                "rating": float(row['rating']),
                "date": str(row['date']),
                "helpful_count": int(row['helpful_count']),
                "verified_purchase": bool(row['verified_purchase']),
                "embedding": embedding
            }
            
            documents.append(doc)
            
            # ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ ë°°ì¹˜ ìƒì„±
            if len(documents) >= batch_size:
                batches.append(documents.copy())
                documents.clear()
            
            # API ì†ë„ ì œí•œ ê³ ë ¤
            if idx % 10 == 0:
                time.sleep(0.1)
        
        # ë§ˆì§€ë§‰ ë°°ì¹˜ ì¶”ê°€
        if documents:
            batches.append(documents)
        
        print(f"âœ“ ë¬¸ì„œ ì¤€ë¹„ ì™„ë£Œ: {len(batches)}ê°œ ë°°ì¹˜")
        return batches
    
    def upload_documents(self, document_batches: List[List[Dict[str, Any]]]) -> None:
        """ë¬¸ì„œ ì—…ë¡œë“œ"""
        
        print(f"â¬†ï¸ ë¬¸ì„œ ì—…ë¡œë“œ ì‹œì‘: {len(document_batches)}ê°œ ë°°ì¹˜")
        
        total_uploaded = 0
        
        for i, batch in enumerate(tqdm(document_batches, desc="ë°°ì¹˜ ì—…ë¡œë“œ")):
            try:
                result = self.search_client.upload_documents(documents=batch)
                
                # ì„±ê³µ/ì‹¤íŒ¨ ì¹´ìš´íŠ¸
                succeeded = sum(1 for r in result if r.succeeded)
                failed = len(batch) - succeeded
                
                total_uploaded += succeeded
                
                if failed > 0:
                    print(f"âš ï¸ ë°°ì¹˜ {i+1}: {succeeded}ê°œ ì„±ê³µ, {failed}ê°œ ì‹¤íŒ¨")
                
                # API ì†ë„ ì œí•œ ê³ ë ¤
                time.sleep(0.5)
                
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ {i+1} ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                continue
        
        print(f"âœ“ ì—…ë¡œë“œ ì™„ë£Œ: {total_uploaded:,}ê°œ ë¬¸ì„œ")
    
    def verify_index(self) -> None:
        """ì¸ë±ìŠ¤ ê²€ì¦"""
        
        print("ğŸ” ì¸ë±ìŠ¤ ê²€ì¦ ì¤‘...")
        
        try:
            # ë¬¸ì„œ ìˆ˜ í™•ì¸
            stats = self.search_client.get_search_index_statistics()
            print(f"âœ“ ì¸ë±ìŠ¤ í†µê³„:")
            print(f"  - ë¬¸ì„œ ìˆ˜: {stats.document_count:,}ê°œ")
            print(f"  - ì €ì¥ í¬ê¸°: {stats.storage_size:,} bytes")
            
            # ìƒ˜í”Œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            results = self.search_client.search(
                search_text="moisturizer",
                top=3,
                select=["product_name", "review_text", "rating"]
            )
            
            print(f"âœ“ ìƒ˜í”Œ ê²€ìƒ‰ ê²°ê³¼:")
            for i, result in enumerate(results, 1):
                print(f"  {i}. {result['product_name'][:50]}... (â˜…{result['rating']})")
            
        except Exception as e:
            print(f"âŒ ì¸ë±ìŠ¤ ê²€ì¦ ì‹¤íŒ¨: {e}")
    
    def migrate(self, csv_path: str, batch_size: int = 50) -> None:
        """ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰"""
        
        print("ğŸš€ Azure AI Search ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
        print("=" * 50)
        
        try:
            # 1. ì¸ë±ìŠ¤ ìƒì„±
            self.create_search_index()
            
            # 2. CSV ë°ì´í„° ë¡œë“œ
            df = self.load_csv_data(csv_path)
            
            # 3. ë¬¸ì„œ ì¤€ë¹„
            document_batches = self.prepare_documents(df, batch_size)
            
            # 4. ë¬¸ì„œ ì—…ë¡œë“œ
            self.upload_documents(document_batches)
            
            # 5. ì¸ë±ìŠ¤ ê²€ì¦
            self.verify_index()
            
            print("=" * 50)
            print("âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
            raise

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    required_vars = [
        "AZURE_SEARCH_ENDPOINT",
        "AZURE_SEARCH_API_KEY", 
        "AZURE_OPENAI_ENDPOINT",
        "AZURE_OPENAI_API_KEY"
    ]
    
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    
    if missing_vars:
        print("âŒ í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤:")
        for var in missing_vars:
            print(f"  - {var}")
        print("\n.env íŒŒì¼ì„ ìƒì„±í•˜ê³  Azure ì„œë¹„ìŠ¤ ì •ë³´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        return
    
    # CSV íŒŒì¼ ê²½ë¡œ í™•ì¸
    csv_files = [
        "results/amazon_reviews.csv",
        "results/kbeauty_reviews.csv", 
        "amazon_reviews.csv",
        "kbeauty_reviews.csv"
    ]
    
    csv_path = None
    for path in csv_files:
        if os.path.exists(path):
            csv_path = path
            break
    
    if not csv_path:
        print("âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ ìœ„ì¹˜ ì¤‘ í•˜ë‚˜ì— CSV íŒŒì¼ì„ ë°°ì¹˜í•˜ì„¸ìš”:")
        for path in csv_files:
            print(f"  - {path}")
        return
    
    # ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
    try:
        migrator = AzureSearchMigrator()
        migrator.migrate(csv_path, batch_size=50)
        
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    main()