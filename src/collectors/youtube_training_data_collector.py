"""
YouTube Training Data Collector
==============================

AI ëª¨ë¸ í•™ìŠµìš© ë°ì´í„°ì…‹ êµ¬ì¶•ì„ ìœ„í•œ YouTube ë°ì´í„° ìˆ˜ì§‘ê¸°

ì£¼ìš” ê¸°ëŠ¥:
- ì†ŒìŠ¤ A: ê±°ì‹œ íŠ¸ë Œë“œ (videos.list - mostPopular)
- ì†ŒìŠ¤ B: í‚¤ì›Œë“œ ë°œêµ´ (search.list - Korean Skincare ë“±)
- ì†ŒìŠ¤ C: íƒ€ê²Ÿ ì±„ë„ (ì£¼ìš” ì¸í”Œë£¨ì–¸ì„œ ì±„ë„ ëª¨ë‹ˆí„°ë§)
- í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (View Velocity, VPV, Engagement Rate)
- CSV ë°ì´í„°ì…‹ ìƒì„± (UTF-8-SIG, ì¼ë³„ ì ì¬)

ì‚¬ìš©ë²•:
    from src.collectors.youtube_training_data_collector import YouTubeTrainingDataCollector
    
    collector = YouTubeTrainingDataCollector(api_keys=['your_api_key'])
    dataset = await collector.collect_daily_dataset()
"""

import asyncio
import aiohttp
import csv
import os
import re
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict
import logging
import json

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class VideoTrainingData:
    """AI í•™ìŠµìš© ì˜ìƒ ë°ì´í„° êµ¬ì¡° (CSV ìŠ¤í‚¤ë§ˆì™€ ì¼ì¹˜)"""
    # ì‹ë³„ì
    collection_date: str  # YYYY-MM-DD
    video_id: str
    
    # ê¸°ë³¸ ì •ë³´
    title: str
    channel_name: str
    upload_date: str  # ISO format
    duration_sec: int
    
    # ì„±ê³¼ ì§€í‘œ
    subscriber_count: int
    view_count: int
    like_count: int
    comment_count: int
    
    # íŒŒìƒ í”¼ì²˜
    view_velocity: float  # ì‹œê°„ë‹¹ ì¡°íšŒìˆ˜ ì¦ê°€ëŸ‰
    vpv_ratio: float     # êµ¬ë…ì ëŒ€ë¹„ ì¡°íšŒìˆ˜ ë¹„ìœ¨
    engagement_rate: float # ì¡°íšŒìˆ˜ ëŒ€ë¹„ ë°˜ì‘ìœ¨
    
    # í…ìŠ¤íŠ¸ ë°ì´í„°
    top_comments_text: str  # íŒŒì´í”„(|)ë¡œ êµ¬ë¶„ëœ ìƒìœ„ ëŒ“ê¸€
    description_keywords: str  # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œ
    
    # íƒ€ê²Ÿê°’
    is_trending_category: int  # ì¸ê¸° ê¸‰ìƒìŠ¹ ì°¨íŠ¸ ì§„ì… ì—¬ë¶€ (1/0)
    
    # ë©”íƒ€ë°ì´í„°
    source_type: str  # 'macro_trend', 'keyword_discovery', 'channel_performance'


class APIQuotaManager:
    """YouTube API í• ë‹¹ëŸ‰ ê´€ë¦¬ì"""
    
    def __init__(self, api_keys: List[str], daily_quota: int = 10000):
        self.api_keys = api_keys
        self.current_key_index = 0
        self.daily_quota = daily_quota
        self.usage_per_key = {key: 0 for key in api_keys}
        self.last_reset = datetime.now().date()
    
    def get_current_api_key(self) -> str:
        """í˜„ì¬ ì‚¬ìš©í•  API í‚¤ ë°˜í™˜"""
        self._check_daily_reset()
        return self.api_keys[self.current_key_index]
    
    def record_usage(self, cost: int):
        """API ì‚¬ìš©ëŸ‰ ê¸°ë¡"""
        current_key = self.get_current_api_key()
        self.usage_per_key[current_key] += cost
        
        # í• ë‹¹ëŸ‰ 90% ì´ˆê³¼ì‹œ ë‹¤ìŒ í‚¤ë¡œ ë¡œí…Œì´ì…˜
        if self.usage_per_key[current_key] > self.daily_quota * 0.9:
            self._rotate_api_key()
    
    def _rotate_api_key(self):
        """API í‚¤ ë¡œí…Œì´ì…˜"""
        old_index = self.current_key_index
        self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)
        
        if self.current_key_index == old_index:
            logger.warning("ëª¨ë“  API í‚¤ì˜ í• ë‹¹ëŸ‰ì´ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤")
            raise Exception("API quota exhausted for all keys")
        
        logger.info(f"API í‚¤ ë¡œí…Œì´ì…˜: {self.current_key_index + 1}/{len(self.api_keys)}")
    
    def _check_daily_reset(self):
        """ì¼ì¼ í• ë‹¹ëŸ‰ ë¦¬ì…‹ í™•ì¸"""
        today = datetime.now().date()
        if today > self.last_reset:
            self.usage_per_key = {key: 0 for key in self.api_keys}
            self.last_reset = today
            self.current_key_index = 0
            logger.info("ì¼ì¼ API í• ë‹¹ëŸ‰ ë¦¬ì…‹")
    
    def get_remaining_quota(self) -> int:
        """ë‚¨ì€ í• ë‹¹ëŸ‰ ë°˜í™˜"""
        current_key = self.get_current_api_key()
        return max(0, self.daily_quota - self.usage_per_key[current_key])


class YouTubeTrainingDataCollector:
    """YouTube AI í•™ìŠµìš© ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    # YouTube Data API v3 ì—”ë“œí¬ì¸íŠ¸
    BASE_URL = "https://www.googleapis.com/youtube/v3"
    
    # SRS ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ í‚¤ì›Œë“œ ì…‹
    TARGET_KEYWORDS = [
        "Korean Skincare", "Glass Skin", "K-Beauty Routine",
        "Korean Beauty", "Korean Makeup", "Korean Cosmetics",
        "Tirtir", "Biodance", "Anua", "COSRX", "Some By Mi",
        "Beauty of Joseon", "Torriden", "Round Lab"
    ]
    
    # ë·°í‹° ê´€ë ¨ í•„í„°ë§ í‚¤ì›Œë“œ
    BEAUTY_FILTER_KEYWORDS = [
        "makeup", "skincare", "beauty", "routine", "review",
        "tutorial", "haul", "unboxing", "korean", "k-beauty",
        "serum", "toner", "moisturizer", "cleanser", "sunscreen"
    ]
    
    # ì£¼ìš” ê¸€ë¡œë²Œ ë·°í‹° ì¸í”Œë£¨ì–¸ì„œ ì±„ë„ ID
    TARGET_CHANNELS = [
        "UCdKuE7a2QZeHPhDntXVZ91w",  # James Welsh
        "UCQhwBjjWuLrcE0tJOjq4rKw",  # Gothamista
        "UC2sYit3cZ2B04MGgy0It6dQ",  # Liah Yoo
        "UCsyn_0Fx8w8eZlASIUkamBg",  # Hyram
        "UCBJycsmduvYEL83R_U4JriQ",  # Mixed Makeup
        # ì‹¤ì œ ìš´ì˜ì‹œ ë” ë§ì€ ì±„ë„ ì¶”ê°€
    ]
    
    def __init__(self, api_keys: List[str], output_dir: str = "results"):
        """
        YouTube í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        
        Args:
            api_keys: YouTube Data API v3 í‚¤ ëª©ë¡
            output_dir: CSV íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        if not api_keys:
            raise ValueError("ìµœì†Œ í•˜ë‚˜ì˜ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        self.quota_manager = APIQuotaManager(api_keys)
        self.session = None
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ìºì‹œ
        self.processed_videos = set()
        
        # íŠ¸ë Œë”© ì˜ìƒ ID ìºì‹œ (is_trending_category íŒë³„ìš©)
        self.trending_video_ids = set()
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        import ssl
        import certifi
        
        # SSL ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ì¸ì¦ì„œ ê²€ì¦ ë¬¸ì œ í•´ê²°)
        ssl_context = ssl.create_default_context(cafile=certifi.where())
        
        # aiohttp ì»¤ë„¥í„° ìƒì„±
        connector = aiohttp.TCPConnector(ssl=ssl_context)
        self.session = aiohttp.ClientSession(connector=connector)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
    
    async def collect_daily_dataset(self, target_date: Optional[str] = None) -> str:
        """
        ì¼ë³„ í•™ìŠµ ë°ì´í„°ì…‹ ìˆ˜ì§‘ ë° CSV ìƒì„±
        
        Args:
            target_date: ìˆ˜ì§‘ ëŒ€ìƒ ë‚ ì§œ (YYYY-MM-DD), Noneì´ë©´ ì˜¤ëŠ˜ ë‚ ì§œ
            
        Returns:
            ìƒì„±ëœ CSV íŒŒì¼ ê²½ë¡œ
        """
        if target_date is None:
            target_date = datetime.now().strftime("%Y-%m-%d")
        
        logger.info(f"ğŸš€ YouTube í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (ë‚ ì§œ: {target_date})")
        
        try:
            # 1ë‹¨ê³„: íŠ¸ë Œë”© ì˜ìƒ ID ìˆ˜ì§‘ (íƒ€ê²Ÿê°’ ìƒì„±ìš©)
            logger.info("ğŸ“Š 1ë‹¨ê³„: íŠ¸ë Œë”© ì˜ìƒ ID ìˆ˜ì§‘")
            await self._collect_trending_video_ids()
            
            # 2ë‹¨ê³„: ë‹¤ì¤‘ ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘
            logger.info("ğŸ” 2ë‹¨ê³„: ë‹¤ì¤‘ ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘")
            
            collection_tasks = [
                self._collect_macro_trends(),      # ì†ŒìŠ¤ A: ê±°ì‹œì  íŠ¸ë Œë“œ
                self._collect_keyword_discovery(), # ì†ŒìŠ¤ B: í‚¤ì›Œë“œ ë°œêµ´
                self._collect_channel_performance() # ì†ŒìŠ¤ C: ì±„ë„ ì„±ê³¼
            ]
            
            macro_data, keyword_data, channel_data = await asyncio.gather(
                *collection_tasks, return_exceptions=True
            )
            
            # ì˜ˆì™¸ ì²˜ë¦¬
            if isinstance(macro_data, Exception):
                logger.error(f"ê±°ì‹œì  íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨: {macro_data}")
                macro_data = []
            
            if isinstance(keyword_data, Exception):
                logger.error(f"í‚¤ì›Œë“œ ë°œêµ´ ì‹¤íŒ¨: {keyword_data}")
                keyword_data = []
            
            if isinstance(channel_data, Exception):
                logger.error(f"ì±„ë„ ì„±ê³¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {channel_data}")
                channel_data = []
            
            # ë°ì´í„° í†µí•©
            all_raw_data = macro_data + keyword_data + channel_data
            logger.info(f"ì›ì‹œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_raw_data)}ê°œ (ê±°ì‹œ {len(macro_data)}, í‚¤ì›Œë“œ {len(keyword_data)}, ì±„ë„ {len(channel_data)})")
            
            if not all_raw_data:
                logger.warning("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return ""
            
            # 3ë‹¨ê³„: ë°ì´í„° ì •ì œ ë° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
            logger.info("âš™ï¸ 3ë‹¨ê³„: ë°ì´í„° ì •ì œ ë° í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§")
            training_data = []
            
            for raw_video in all_raw_data:
                try:
                    processed_video = await self._process_video_for_training(raw_video, target_date)
                    if processed_video:
                        training_data.append(processed_video)
                except Exception as e:
                    logger.error(f"ì˜ìƒ ì²˜ë¦¬ ì‹¤íŒ¨ ({raw_video.get('id', 'unknown')}): {e}")
                    continue
            
            # ì¤‘ë³µ ì œê±° (video_id ê¸°ì¤€)
            unique_training_data = self._deduplicate_training_data(training_data)
            logger.info(f"ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {len(unique_training_data)}ê°œ (ì¤‘ë³µ ì œê±° í›„)")
            
            # 4ë‹¨ê³„: CSV íŒŒì¼ ìƒì„±
            logger.info("ğŸ’¾ 4ë‹¨ê³„: CSV ë°ì´í„°ì…‹ ìƒì„±")
            csv_path = await self._save_training_dataset_csv(unique_training_data, target_date)
            
            logger.info(f"âœ… í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {csv_path}")
            logger.info(f"   - ì´ ë ˆì½”ë“œ ìˆ˜: {len(unique_training_data)}")
            logger.info(f"   - íŠ¸ë Œë”© ì˜ìƒ ìˆ˜: {sum(1 for data in unique_training_data if data.is_trending_category == 1)}")
            
            return csv_path
            
        except Exception as e:
            logger.error(f"í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return ""
    
    async def _collect_trending_video_ids(self):
        """íŠ¸ë Œë”© ì˜ìƒ ID ìˆ˜ì§‘ (íƒ€ê²Ÿê°’ ìƒì„±ìš©)"""
        try:
            api_key = self.quota_manager.get_current_api_key()
            
            params = {
                'part': 'id',
                'chart': 'mostPopular',
                'regionCode': 'US',
                'categoryId': '26',  # Howto & Style (ë·°í‹° í¬í•¨)
                'maxResults': 50,
                'key': api_key
            }
            
            url = f"{self.BASE_URL}/videos"
            
            async with self.session.get(url, params=params) as response:
                self.quota_manager.record_usage(1)  # Videos API ë¹„ìš©
                
                if response.status == 200:
                    data = await response.json()
                    video_ids = [item['id'] for item in data.get('items', [])]
                    self.trending_video_ids.update(video_ids)
                    logger.info(f"íŠ¸ë Œë”© ì˜ìƒ ID {len(video_ids)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
                else:
                    logger.error(f"íŠ¸ë Œë”© ì˜ìƒ ID ìˆ˜ì§‘ ì‹¤íŒ¨: {response.status}")
                    
        except Exception as e:
            logger.error(f"íŠ¸ë Œë”© ì˜ìƒ ID ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    async def _collect_macro_trends(self) -> List[Dict[str, Any]]:
        """ì†ŒìŠ¤ A: ê±°ì‹œì  íŠ¸ë Œë“œ ê°ì§€"""
        try:
            api_key = self.quota_manager.get_current_api_key()
            
            params = {
                'part': 'snippet,statistics,contentDetails',
                'chart': 'mostPopular',
                'regionCode': 'US',
                'categoryId': '26',  # Howto & Style
                'maxResults': 50,
                'key': api_key
            }
            
            url = f"{self.BASE_URL}/videos"
            
            async with self.session.get(url, params=params) as response:
                self.quota_manager.record_usage(1)
                
                if response.status == 200:
                    data = await response.json()
                    videos = data.get('items', [])
                    
                    # ë·°í‹° ê´€ë ¨ ì˜ìƒë§Œ í•„í„°ë§
                    filtered_videos = []
                    for video in videos:
                        if self._is_beauty_related(video):
                            video['source_type'] = 'macro_trend'
                            filtered_videos.append(video)
                    
                    logger.info(f"ê±°ì‹œì  íŠ¸ë Œë“œ ìˆ˜ì§‘: {len(filtered_videos)}ê°œ (ì „ì²´ {len(videos)}ê°œ ì¤‘)")
                    return filtered_videos
                else:
                    logger.error(f"ê±°ì‹œì  íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨: {response.status}")
                    return []
                    
        except Exception as e:
            logger.error(f"ê±°ì‹œì  íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    async def _collect_keyword_discovery(self) -> List[Dict[str, Any]]:
        """ì†ŒìŠ¤ B: ë§ˆì´í¬ë¡œ í‚¤ì›Œë“œ ë°œêµ´"""
        all_videos = []
        
        for keyword in self.TARGET_KEYWORDS:
            try:
                api_key = self.quota_manager.get_current_api_key()
                
                # ê²€ìƒ‰ API í˜¸ì¶œ
                search_params = {
                    'part': 'snippet',
                    'q': keyword,
                    'type': 'video',
                    'maxResults': 20,
                    'order': 'viewCount',
                    'publishedAfter': (datetime.now() - timedelta(days=7)).isoformat() + 'Z',
                    'key': api_key
                }
                
                search_url = f"{self.BASE_URL}/search"
                
                async with self.session.get(search_url, params=search_params) as response:
                    self.quota_manager.record_usage(100)  # Search API ë¹„ìš©
                    
                    if response.status != 200:
                        logger.error(f"í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {response.status}")
                        continue
                    
                    search_data = await response.json()
                    video_ids = [item['id']['videoId'] for item in search_data.get('items', [])]
                    
                    if not video_ids:
                        continue
                
                # ì˜ìƒ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                videos_params = {
                    'part': 'snippet,statistics,contentDetails',
                    'id': ','.join(video_ids),
                    'key': api_key
                }
                
                videos_url = f"{self.BASE_URL}/videos"
                
                async with self.session.get(videos_url, params=videos_params) as response:
                    self.quota_manager.record_usage(1)
                    
                    if response.status == 200:
                        data = await response.json()
                        videos = data.get('items', [])
                        
                        for video in videos:
                            video['source_type'] = 'keyword_discovery'
                            video['discovered_keyword'] = keyword
                        
                        all_videos.extend(videos)
                        logger.debug(f"í‚¤ì›Œë“œ '{keyword}': {len(videos)}ê°œ ì˜ìƒ ìˆ˜ì§‘")
                    
                # API í˜¸ì¶œ ê°„ ë”œë ˆì´
                await asyncio.sleep(0.1)
                
            except Exception as e:
                logger.error(f"í‚¤ì›Œë“œ '{keyword}' ë°œêµ´ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"í‚¤ì›Œë“œ ë°œêµ´ ì™„ë£Œ: {len(all_videos)}ê°œ ì˜ìƒ")
        return all_videos
    
    async def _collect_channel_performance(self) -> List[Dict[str, Any]]:
        """ì†ŒìŠ¤ C: íƒ€ê²Ÿ ì±„ë„ ì„±ê³¼ ê¸°ë°˜ ê°ì§€"""
        all_videos = []
        
        for channel_id in self.TARGET_CHANNELS:
            try:
                api_key = self.quota_manager.get_current_api_key()
                
                # ì±„ë„ì˜ ì—…ë¡œë“œ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ID ê°€ì ¸ì˜¤ê¸°
                channel_params = {
                    'part': 'contentDetails',
                    'id': channel_id,
                    'key': api_key
                }
                
                channel_url = f"{self.BASE_URL}/channels"
                
                async with self.session.get(channel_url, params=channel_params) as response:
                    self.quota_manager.record_usage(1)
                    
                    if response.status != 200:
                        logger.error(f"ì±„ë„ {channel_id} ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {response.status}")
                        continue
                    
                    channel_data = await response.json()
                    items = channel_data.get('items', [])
                    
                    if not items:
                        continue
                    
                    uploads_playlist_id = items[0]['contentDetails']['relatedPlaylists']['uploads']
                
                # ìµœì‹  ì—…ë¡œë“œ ì˜ìƒ ê°€ì ¸ì˜¤ê¸°
                playlist_params = {
                    'part': 'snippet',
                    'playlistId': uploads_playlist_id,
                    'maxResults': 10,
                    'key': api_key
                }
                
                playlist_url = f"{self.BASE_URL}/playlistItems"
                
                async with self.session.get(playlist_url, params=playlist_params) as response:
                    self.quota_manager.record_usage(1)
                    
                    if response.status != 200:
                        continue
                    
                    playlist_data = await response.json()
                    video_ids = [
                        item['snippet']['resourceId']['videoId'] 
                        for item in playlist_data.get('items', [])
                    ]
                    
                    if not video_ids:
                        continue
                
                # ì˜ìƒ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                videos_params = {
                    'part': 'snippet,statistics,contentDetails',
                    'id': ','.join(video_ids),
                    'key': api_key
                }
                
                videos_url = f"{self.BASE_URL}/videos"
                
                async with self.session.get(videos_url, params=videos_params) as response:
                    self.quota_manager.record_usage(1)
                    
                    if response.status == 200:
                        data = await response.json()
                        videos = data.get('items', [])
                        
                        # ë·°í‹° ê´€ë ¨ ì˜ìƒë§Œ í•„í„°ë§
                        filtered_videos = []
                        for video in videos:
                            if self._is_beauty_related(video):
                                video['source_type'] = 'channel_performance'
                                video['monitored_channel_id'] = channel_id
                                filtered_videos.append(video)
                        
                        all_videos.extend(filtered_videos)
                        logger.debug(f"ì±„ë„ {channel_id}: {len(filtered_videos)}ê°œ ë·°í‹° ì˜ìƒ ìˆ˜ì§‘")
                
                # API í˜¸ì¶œ ê°„ ë”œë ˆì´
                await asyncio.sleep(0.1)
                
            except Exception as e:
                logger.error(f"ì±„ë„ {channel_id} ì„±ê³¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"ì±„ë„ ì„±ê³¼ ìˆ˜ì§‘ ì™„ë£Œ: {len(all_videos)}ê°œ ì˜ìƒ")
        return all_videos
    
    def _is_beauty_related(self, video: Dict[str, Any]) -> bool:
        """ì˜ìƒì´ ë·°í‹° ê´€ë ¨ì¸ì§€ íŒë³„"""
        try:
            snippet = video.get('snippet', {})
            title = snippet.get('title', '').lower()
            description = snippet.get('description', '').lower()
            tags = [tag.lower() for tag in snippet.get('tags', [])]
            
            # ì œëª©, ì„¤ëª…, íƒœê·¸ì—ì„œ ë·°í‹° í‚¤ì›Œë“œ ê²€ìƒ‰
            text_to_check = f"{title} {description} {' '.join(tags)}"
            
            for keyword in self.BEAUTY_FILTER_KEYWORDS:
                if keyword in text_to_check:
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"ë·°í‹° ê´€ë ¨ì„± íŒë³„ ì‹¤íŒ¨: {e}")
            return False
    
    async def _process_video_for_training(self, raw_video: Dict[str, Any], collection_date: str) -> Optional[VideoTrainingData]:
        """ì›ì‹œ ì˜ìƒ ë°ì´í„°ë¥¼ í•™ìŠµìš© ë°ì´í„°ë¡œ ë³€í™˜"""
        try:
            snippet = raw_video.get('snippet', {})
            statistics = raw_video.get('statistics', {})
            content_details = raw_video.get('contentDetails', {})
            
            video_id = raw_video.get('id', '')
            if not video_id:
                return None
            
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            title = self._clean_text(snippet.get('title', ''))
            channel_name = self._clean_text(snippet.get('channelTitle', ''))
            upload_date = snippet.get('publishedAt', '')
            
            # ì˜ìƒ ê¸¸ì´ íŒŒì‹± (PT4M13S -> 253ì´ˆ)
            duration_sec = self._parse_duration(content_details.get('duration', 'PT0S'))
            
            # í†µê³„ ì •ë³´ ì¶”ì¶œ (ê²°ì¸¡ì¹˜ ì²˜ë¦¬)
            view_count = int(statistics.get('viewCount', 0))
            like_count = int(statistics.get('likeCount', -1)) if statistics.get('likeCount') is not None else -1
            comment_count = int(statistics.get('commentCount', -1)) if statistics.get('commentCount') is not None else -1
            
            # ì±„ë„ êµ¬ë…ì ìˆ˜ ì¡°íšŒ (ë³„ë„ API í˜¸ì¶œ í•„ìš”)
            subscriber_count = await self._get_channel_subscriber_count(snippet.get('channelId', ''))
            
            # íŒŒìƒ í”¼ì²˜ ê³„ì‚°
            view_velocity = self._calculate_view_velocity(view_count, upload_date)
            vpv_ratio = self._calculate_vpv_ratio(view_count, subscriber_count)
            engagement_rate = self._calculate_engagement_rate(view_count, like_count, comment_count)
            
            # ëŒ“ê¸€ ë°ì´í„° ìˆ˜ì§‘
            top_comments = await self._get_top_comments(video_id)
            top_comments_text = '|'.join(top_comments) if top_comments else ''
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            description_keywords = self._extract_keywords(snippet.get('description', ''))
            
            # íƒ€ê²Ÿê°’ ì„¤ì • (íŠ¸ë Œë”© ì°¨íŠ¸ ì§„ì… ì—¬ë¶€)
            is_trending_category = 1 if video_id in self.trending_video_ids else 0
            
            return VideoTrainingData(
                collection_date=collection_date,
                video_id=video_id,
                title=title,
                channel_name=channel_name,
                upload_date=upload_date,
                duration_sec=duration_sec,
                subscriber_count=subscriber_count,
                view_count=view_count,
                like_count=like_count,
                comment_count=comment_count,
                view_velocity=view_velocity,
                vpv_ratio=vpv_ratio,
                engagement_rate=engagement_rate,
                top_comments_text=top_comments_text,
                description_keywords=description_keywords,
                is_trending_category=is_trending_category,
                source_type=raw_video.get('source_type', 'unknown')
            )
            
        except Exception as e:
            logger.error(f"ì˜ìƒ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ (CSV ê¹¨ì§ ë°©ì§€)"""
        if not text:
            return ""
        
        # ì´ëª¨ì§€, íŠ¹ìˆ˜ë¬¸ì, ê°œí–‰ë¬¸ì ì œê±°/ì¹˜í™˜
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text = re.sub(r'\s+', ' ', text)  # ì—°ì† ê³µë°± ì œê±°
        text = text.strip()
        
        return text
    
    def _parse_duration(self, duration_str: str) -> int:
        """YouTube ì˜ìƒ ê¸¸ì´ë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜ (PT4M13S -> 253)"""
        try:
            if not duration_str or duration_str == 'PT0S':
                return 0
            
            # PT4M13S í˜•íƒœ íŒŒì‹±
            pattern = r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?'
            match = re.match(pattern, duration_str)
            
            if not match:
                return 0
            
            hours = int(match.group(1) or 0)
            minutes = int(match.group(2) or 0)
            seconds = int(match.group(3) or 0)
            
            return hours * 3600 + minutes * 60 + seconds
            
        except Exception as e:
            logger.error(f"ì˜ìƒ ê¸¸ì´ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return 0
    
    async def _get_channel_subscriber_count(self, channel_id: str) -> int:
        """ì±„ë„ êµ¬ë…ì ìˆ˜ ì¡°íšŒ"""
        try:
            if not channel_id:
                return 0
            
            api_key = self.quota_manager.get_current_api_key()
            
            params = {
                'part': 'statistics',
                'id': channel_id,
                'key': api_key
            }
            
            url = f"{self.BASE_URL}/channels"
            
            async with self.session.get(url, params=params) as response:
                self.quota_manager.record_usage(1)
                
                if response.status == 200:
                    data = await response.json()
                    items = data.get('items', [])
                    
                    if items:
                        stats = items[0].get('statistics', {})
                        return int(stats.get('subscriberCount', 0))
                
                return 0
                
        except Exception as e:
            logger.error(f"ì±„ë„ êµ¬ë…ì ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 0
    
    def _calculate_view_velocity(self, view_count: int, upload_date: str) -> float:
        """ì‹œê°„ë‹¹ ì¡°íšŒìˆ˜ ì¦ê°€ëŸ‰ ê³„ì‚°"""
        try:
            if not upload_date:
                return 0.0
            
            # ì—…ë¡œë“œ ì‹œê°„ê³¼ í˜„ì¬ ì‹œê°„ì˜ ì°¨ì´ ê³„ì‚°
            upload_time = datetime.fromisoformat(upload_date.replace('Z', '+00:00'))
            current_time = datetime.now(upload_time.tzinfo)
            
            hours_elapsed = (current_time - upload_time).total_seconds() / 3600
            
            if hours_elapsed <= 0:
                return 0.0
            
            return view_count / hours_elapsed
            
        except Exception as e:
            logger.error(f"ì¡°íšŒìˆ˜ ì†ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_vpv_ratio(self, view_count: int, subscriber_count: int) -> float:
        """êµ¬ë…ì ëŒ€ë¹„ ì¡°íšŒìˆ˜ ë¹„ìœ¨ ê³„ì‚°"""
        try:
            if subscriber_count <= 0:
                return 0.0
            
            return view_count / subscriber_count
            
        except Exception as e:
            logger.error(f"VPV ë¹„ìœ¨ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_engagement_rate(self, view_count: int, like_count: int, comment_count: int) -> float:
        """ì°¸ì—¬ìœ¨ ê³„ì‚°"""
        try:
            if view_count <= 0:
                return 0.0
            
            # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (-1ì¸ ê²½ìš° 0ìœ¼ë¡œ ì²˜ë¦¬)
            likes = max(0, like_count)
            comments = max(0, comment_count)
            
            engagement = likes + comments
            return engagement / view_count
            
        except Exception as e:
            logger.error(f"ì°¸ì—¬ìœ¨ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    async def _get_top_comments(self, video_id: str, max_comments: int = 30) -> List[str]:
        """ìƒìœ„ ëŒ“ê¸€ ìˆ˜ì§‘"""
        try:
            api_key = self.quota_manager.get_current_api_key()
            
            params = {
                'part': 'snippet',
                'videoId': video_id,
                'maxResults': max_comments,
                'order': 'relevance',
                'key': api_key
            }
            
            url = f"{self.BASE_URL}/commentThreads"
            
            async with self.session.get(url, params=params) as response:
                self.quota_manager.record_usage(1)
                
                if response.status == 200:
                    data = await response.json()
                    comments = []
                    
                    for item in data.get('items', []):
                        comment_text = item['snippet']['topLevelComment']['snippet']['textDisplay']
                        clean_comment = self._clean_text(comment_text)
                        if clean_comment:
                            comments.append(clean_comment)
                    
                    return comments
                else:
                    return []
                    
        except Exception as e:
            logger.error(f"ëŒ“ê¸€ ìˆ˜ì§‘ ì‹¤íŒ¨ ({video_id}): {e}")
            return []
    
    def _extract_keywords(self, description: str) -> str:
        """ì„¤ëª…ë€ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            if not description:
                return ""
            
            # ë·°í‹° ê´€ë ¨ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
            found_keywords = []
            description_lower = description.lower()
            
            for keyword in self.BEAUTY_FILTER_KEYWORDS:
                if keyword in description_lower:
                    found_keywords.append(keyword)
            
            # ë¸Œëœë“œëª… ì¶”ì¶œ
            for keyword in self.TARGET_KEYWORDS:
                if keyword.lower() in description_lower:
                    found_keywords.append(keyword)
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            unique_keywords = list(set(found_keywords))
            return ', '.join(sorted(unique_keywords))
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""
    
    def _deduplicate_training_data(self, training_data: List[VideoTrainingData]) -> List[VideoTrainingData]:
        """ì¤‘ë³µ ë°ì´í„° ì œê±° (video_id ê¸°ì¤€)"""
        seen_ids = set()
        unique_data = []
        
        for data in training_data:
            if data.video_id not in seen_ids:
                seen_ids.add(data.video_id)
                unique_data.append(data)
        
        return unique_data
    
    async def _save_training_dataset_csv(self, training_data: List[VideoTrainingData], target_date: str) -> str:
        """í•™ìŠµ ë°ì´í„°ì…‹ì„ CSV íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # íŒŒì¼ëª… ìƒì„± (SRS ìš”êµ¬ì‚¬í•­ì— ë”°ë¼)
            filename = f"youtube_viral_dataset_v1_{target_date.replace('-', '')}.csv"
            csv_path = os.path.join(self.output_dir, filename)
            
            # CSV í—¤ë” ì •ì˜ (SRS ìŠ¤í‚¤ë§ˆì™€ ì¼ì¹˜)
            fieldnames = [
                'collection_date', 'video_id', 'title', 'channel_name', 'upload_date', 'duration_sec',
                'subscriber_count', 'view_count', 'like_count', 'comment_count',
                'view_velocity', 'vpv_ratio', 'engagement_rate',
                'top_comments_text', 'description_keywords', 'is_trending_category', 'source_type'
            ]
            
            # UTF-8-SIG ì¸ì½”ë”©ìœ¼ë¡œ CSV ì €ì¥ (Excel í˜¸í™˜)
            with open(csv_path, 'w', encoding='utf-8-sig', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                
                for data in training_data:
                    # dataclassë¥¼ dictë¡œ ë³€í™˜
                    row = asdict(data)
                    writer.writerow(row)
            
            # íŒŒì¼ ì •ë³´ ë¡œê¹…
            file_size = os.path.getsize(csv_path)
            logger.info(f"CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ:")
            logger.info(f"  - íŒŒì¼ ê²½ë¡œ: {csv_path}")
            logger.info(f"  - íŒŒì¼ í¬ê¸°: {file_size:,} bytes")
            logger.info(f"  - ë ˆì½”ë“œ ìˆ˜: {len(training_data)}")
            
            # ë°ì´í„° í’ˆì§ˆ ìš”ì•½
            trending_count = sum(1 for data in training_data if data.is_trending_category == 1)
            avg_view_count = sum(data.view_count for data in training_data) / len(training_data) if training_data else 0
            avg_engagement = sum(data.engagement_rate for data in training_data) / len(training_data) if training_data else 0
            
            logger.info(f"ë°ì´í„° í’ˆì§ˆ ìš”ì•½:")
            logger.info(f"  - íŠ¸ë Œë”© ì˜ìƒ: {trending_count}/{len(training_data)} ({trending_count/len(training_data)*100:.1f}%)")
            logger.info(f"  - í‰ê·  ì¡°íšŒìˆ˜: {avg_view_count:,.0f}")
            logger.info(f"  - í‰ê·  ì°¸ì—¬ìœ¨: {avg_engagement:.4f}")
            
            return csv_path
            
        except Exception as e:
            logger.error(f"CSV ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def main():
    """ì‚¬ìš© ì˜ˆì‹œ"""
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ
    api_keys_str = os.getenv("YOUTUBE_API_KEYS", "")
    api_keys = [key.strip() for key in api_keys_str.split(",") if key.strip()]
    
    if not api_keys:
        logger.error("YOUTUBE_API_KEYS í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”")
        return
    
    # í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ë° ì‹¤í–‰
    async with YouTubeTrainingDataCollector(api_keys, "results") as collector:
        logger.info("YouTube í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        
        # ì˜¤ëŠ˜ ë‚ ì§œë¡œ ë°ì´í„°ì…‹ ìƒì„±
        csv_path = await collector.collect_daily_dataset()
        
        if csv_path:
            logger.info(f"âœ… í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± ì„±ê³µ: {csv_path}")
            
            # ìƒì„±ëœ CSV íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°
            try:
                import pandas as pd
                df = pd.read_csv(csv_path)
                logger.info(f"ğŸ“Š ë°ì´í„°ì…‹ ë¯¸ë¦¬ë³´ê¸°:")
                logger.info(f"   - ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
                logger.info(f"   - í–‰ ìˆ˜: {len(df)}")
                logger.info(f"   - ì»¬ëŸ¼ ëª©ë¡: {list(df.columns)}")
                
                if len(df) > 0:
                    logger.info(f"   - ì²« ë²ˆì§¸ ë ˆì½”ë“œ ì œëª©: {df.iloc[0]['title']}")
                    logger.info(f"   - ìµœê³  ì¡°íšŒìˆ˜: {df['view_count'].max():,}")
                    logger.info(f"   - íŠ¸ë Œë”© ë¹„ìœ¨: {df['is_trending_category'].mean():.2%}")
                    
            except ImportError:
                logger.info("pandasê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ë¯¸ë¦¬ë³´ê¸°ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
            except Exception as e:
                logger.error(f"ë°ì´í„°ì…‹ ë¯¸ë¦¬ë³´ê¸° ì‹¤íŒ¨: {e}")
        else:
            logger.error("âŒ í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨")


if __name__ == "__main__":
    asyncio.run(main())