#!/usr/bin/env python3
"""
YouTube AI í•™ìŠµìš© ë°ì´í„°ì…‹ ë¶„ì„ê¸°
===============================

ìˆ˜ì§‘ëœ CSV ë°ì´í„°ì…‹ì„ ë¶„ì„í•˜ì—¬ ë°ì´í„° í’ˆì§ˆ, K-Beauty ê´€ë ¨ì„±, 
ì„±ê³¼ ì§€í‘œ ë“±ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python analyze_dataset.py                           # ìµœì‹  íŒŒì¼ ìë™ ë¶„ì„
    python analyze_dataset.py --file dataset.csv       # íŠ¹ì • íŒŒì¼ ë¶„ì„
    python analyze_dataset.py --all                    # ëª¨ë“  íŒŒì¼ ë¶„ì„
"""

import csv
import argparse
import os
import glob
from collections import Counter
import statistics
from datetime import datetime
import sys


class YouTubeDatasetAnalyzer:
    """YouTube ë°ì´í„°ì…‹ ë¶„ì„ê¸°"""
    
    def __init__(self):
        # K-Beauty ê´€ë ¨ í‚¤ì›Œë“œ ì •ì˜
        self.kbeauty_keywords = [
            'korean', 'k-beauty', 'skincare', 'beauty', 'makeup', 'cosmetics',
            'tirtir', 'biodance', 'anua', 'cosrx', 'some by mi', 'beauty of joseon',
            'torriden', 'round lab', 'glass skin', 'routine', 'serum', 'toner',
            'moisturizer', 'cleanser', 'sunscreen', 'mask', 'essence', 'cream',
            'lotion', 'ampoule', 'patch', 'peel', 'exfoliant', 'mist', 'oil'
        ]
    
    def load_dataset(self, file_path: str) -> list:
        """CSV ë°ì´í„°ì…‹ ë¡œë“œ"""
        try:
            with open(file_path, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                data = list(reader)
            return data
        except Exception as e:
            print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def analyze_basic_stats(self, data: list) -> dict:
        """ê¸°ë³¸ í†µê³„ ë¶„ì„"""
        if not data:
            return {}
        
        return {
            'total_records': len(data),
            'total_columns': len(data[0].keys()),
            'columns': list(data[0].keys()),
            'file_size_kb': len(str(data)) / 1024
        }
    
    def analyze_data_sources(self, data: list) -> dict:
        """ë°ì´í„° ì†ŒìŠ¤ë³„ ë¶„í¬ ë¶„ì„"""
        source_counts = Counter(row['source_type'] for row in data)
        total = len(data)
        
        return {
            'source_distribution': {
                source: {
                    'count': count,
                    'percentage': count / total * 100
                }
                for source, count in source_counts.items()
            }
        }
    
    def analyze_trending_videos(self, data: list) -> dict:
        """íŠ¸ë Œë”© ì˜ìƒ ë¶„ì„"""
        trending_count = sum(1 for row in data if row['is_trending_category'] == '1')
        total = len(data)
        
        return {
            'trending_count': trending_count,
            'trending_percentage': trending_count / total * 100,
            'normal_count': total - trending_count,
            'normal_percentage': (total - trending_count) / total * 100
        }
    
    def analyze_performance_metrics(self, data: list) -> dict:
        """ì„±ê³¼ ì§€í‘œ ë¶„ì„"""
        # ì¡°íšŒìˆ˜ ë¶„ì„
        view_counts = []
        engagement_rates = []
        vpv_ratios = []
        velocities = []
        subscriber_counts = []
        durations = []
        
        for row in data:
            try:
                view_counts.append(int(row['view_count']))
                engagement_rates.append(float(row['engagement_rate']))
                vpv_ratios.append(float(row['vpv_ratio']))
                velocities.append(float(row['view_velocity']))
                subscriber_counts.append(int(row['subscriber_count']))
                durations.append(int(row['duration_sec']))
            except (ValueError, KeyError):
                continue
        
        def safe_stats(values):
            if not values:
                return {'mean': 0, 'median': 0, 'max': 0, 'min': 0}
            return {
                'mean': statistics.mean(values),
                'median': statistics.median(values),
                'max': max(values),
                'min': min(values)
            }
        
        return {
            'view_counts': safe_stats(view_counts),
            'engagement_rates': safe_stats(engagement_rates),
            'vpv_ratios': safe_stats(vpv_ratios),
            'velocities': safe_stats(velocities),
            'subscriber_counts': safe_stats(subscriber_counts),
            'durations': safe_stats(durations),
            'high_vpv_count': sum(1 for vpv in vpv_ratios if vpv > 2.0)
        }
    
    def analyze_kbeauty_relevance(self, data: list) -> dict:
        """K-Beauty ê´€ë ¨ì„± ë¶„ì„"""
        kbeauty_count = 0
        keyword_matches = Counter()
        kbeauty_samples = []
        
        for row in data:
            title = row['title'].lower()
            description_keywords = row['description_keywords'].lower()
            
            # K-Beauty í‚¤ì›Œë“œ ì°¾ê¸°
            found_keywords = []
            for keyword in self.kbeauty_keywords:
                if keyword in title or keyword in description_keywords:
                    found_keywords.append(keyword)
                    keyword_matches[keyword] += 1
            
            if found_keywords:
                kbeauty_count += 1
                try:
                    kbeauty_samples.append({
                        'title': row['title'],
                        'channel': row['channel_name'],
                        'views': int(row['view_count']),
                        'keywords': row['description_keywords'],
                        'found_keywords': found_keywords
                    })
                except (ValueError, KeyError):
                    pass
        
        # ì¡°íšŒìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        kbeauty_samples.sort(key=lambda x: x['views'], reverse=True)
        
        return {
            'kbeauty_count': kbeauty_count,
            'kbeauty_percentage': kbeauty_count / len(data) * 100,
            'normal_count': len(data) - kbeauty_count,
            'normal_percentage': (len(data) - kbeauty_count) / len(data) * 100,
            'top_keywords': keyword_matches.most_common(10),
            'top_samples': kbeauty_samples[:5]
        }
    
    def analyze_data_quality(self, data: list) -> dict:
        """ë°ì´í„° í’ˆì§ˆ ë¶„ì„"""
        missing_data = 0
        required_fields = ['title', 'channel_name', 'view_count', 'video_id']
        
        for row in data:
            for field in required_fields:
                if not row.get(field, '').strip():
                    missing_data += 1
                    break
        
        return {
            'missing_data_count': missing_data,
            'missing_data_percentage': missing_data / len(data) * 100,
            'completeness_percentage': (len(data) - missing_data) / len(data) * 100
        }
    
    def analyze_high_performance(self, data: list) -> dict:
        """ê³ ì„±ê³¼ ì˜ìƒ ë¶„ì„"""
        high_performance = []
        
        for row in data:
            try:
                vpv = float(row['vpv_ratio'])
                engagement = float(row['engagement_rate'])
                velocity = float(row['view_velocity'])
                views = int(row['view_count'])
                
                # ê³ ì„±ê³¼ ê¸°ì¤€: VPV > 2.0 ë˜ëŠ” ì°¸ì—¬ìœ¨ > 5% ë˜ëŠ” ì‹œê°„ë‹¹ ì¡°íšŒìˆ˜ > 1000
                if vpv > 2.0 or engagement > 0.05 or velocity > 1000:
                    high_performance.append({
                        'title': row['title'],
                        'channel': row['channel_name'],
                        'vpv': vpv,
                        'engagement': engagement,
                        'velocity': velocity,
                        'views': views,
                        'is_trending': row['is_trending_category'] == '1'
                    })
            except (ValueError, KeyError):
                continue
        
        # ì¡°íšŒìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        high_performance.sort(key=lambda x: x['views'], reverse=True)
        
        return {
            'high_performance_count': len(high_performance),
            'high_performance_percentage': len(high_performance) / len(data) * 100,
            'top_performers': high_performance[:5]
        }
    
    def analyze_channels(self, data: list) -> dict:
        """ì±„ë„ ë¶„ì„"""
        channels = [row['channel_name'] for row in data if row['channel_name']]
        channel_counts = Counter(channels)
        
        return {
            'unique_channels': len(set(channels)),
            'top_channels': channel_counts.most_common(5)
        }
    
    def generate_report(self, file_path: str) -> None:
        """ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        print(f"ğŸ” YouTube AI í•™ìŠµìš© ë°ì´í„°ì…‹ ë¶„ì„ ë¦¬í¬íŠ¸")
        print(f"ğŸ“ íŒŒì¼: {file_path}")
        print(f"ğŸ“… ë¶„ì„ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)
        
        # ë°ì´í„° ë¡œë“œ
        data = self.load_dataset(file_path)
        if not data:
            print("âŒ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 1. ê¸°ë³¸ í†µê³„
        basic_stats = self.analyze_basic_stats(data)
        print(f"\nğŸ“Š 1. ê¸°ë³¸ í†µê³„")
        print(f"   - ì´ ë ˆì½”ë“œ ìˆ˜: {basic_stats['total_records']:,}ê°œ")
        print(f"   - ì»¬ëŸ¼ ìˆ˜: {basic_stats['total_columns']}ê°œ")
        print(f"   - ì˜ˆìƒ íŒŒì¼ í¬ê¸°: {basic_stats['file_size_kb']:.1f} KB")
        
        # 2. ë°ì´í„° ì†ŒìŠ¤ ë¶„í¬
        source_analysis = self.analyze_data_sources(data)
        print(f"\nğŸ“ˆ 2. ë°ì´í„° ì†ŒìŠ¤ë³„ ë¶„í¬")
        for source, stats in source_analysis['source_distribution'].items():
            print(f"   - {source}: {stats['count']}ê°œ ({stats['percentage']:.1f}%)")
        
        # 3. íŠ¸ë Œë”© ì˜ìƒ ë¶„ì„
        trending_analysis = self.analyze_trending_videos(data)
        print(f"\nğŸ”¥ 3. íŠ¸ë Œë”© ì˜ìƒ ë¶„ì„")
        print(f"   - íŠ¸ë Œë”© ì˜ìƒ: {trending_analysis['trending_count']}ê°œ ({trending_analysis['trending_percentage']:.1f}%)")
        print(f"   - ì¼ë°˜ ì˜ìƒ: {trending_analysis['normal_count']}ê°œ ({trending_analysis['normal_percentage']:.1f}%)")
        
        # 4. ì„±ê³¼ ì§€í‘œ ë¶„ì„
        performance = self.analyze_performance_metrics(data)
        print(f"\nğŸ“Š 4. ì„±ê³¼ ì§€í‘œ ë¶„ì„")
        
        print(f"   ğŸ‘€ ì¡°íšŒìˆ˜:")
        print(f"      - í‰ê· : {performance['view_counts']['mean']:,.0f}")
        print(f"      - ì¤‘ê°„ê°’: {performance['view_counts']['median']:,.0f}")
        print(f"      - ìµœê³ : {performance['view_counts']['max']:,.0f}")
        print(f"      - ìµœì €: {performance['view_counts']['min']:,.0f}")
        
        print(f"   ğŸ’¬ ì°¸ì—¬ìœ¨:")
        print(f"      - í‰ê· : {performance['engagement_rates']['mean']:.4f} ({performance['engagement_rates']['mean']*100:.2f}%)")
        print(f"      - ì¤‘ê°„ê°’: {performance['engagement_rates']['median']:.4f} ({performance['engagement_rates']['median']*100:.2f}%)")
        print(f"      - ìµœê³ : {performance['engagement_rates']['max']:.4f} ({performance['engagement_rates']['max']*100:.2f}%)")
        
        print(f"   ğŸ“Š VPV (êµ¬ë…ì ëŒ€ë¹„ ì¡°íšŒìˆ˜):")
        print(f"      - í‰ê· : {performance['vpv_ratios']['mean']:.3f}")
        print(f"      - ì¤‘ê°„ê°’: {performance['vpv_ratios']['median']:.3f}")
        print(f"      - ìµœê³ : {performance['vpv_ratios']['max']:.3f}")
        print(f"      - VPV > 2.0 (ì´ˆê°•ë ¥ ë°”ì´ëŸ´): {performance['high_vpv_count']}ê°œ")
        
        print(f"   âš¡ View Velocity (ì‹œê°„ë‹¹ ì¡°íšŒìˆ˜):")
        print(f"      - í‰ê· : {performance['velocities']['mean']:,.0f} views/hour")
        print(f"      - ì¤‘ê°„ê°’: {performance['velocities']['median']:,.0f} views/hour")
        print(f"      - ìµœê³ : {performance['velocities']['max']:,.0f} views/hour")
        
        print(f"   ğŸ“º ì±„ë„ êµ¬ë…ì:")
        print(f"      - í‰ê· : {performance['subscriber_counts']['mean']:,.0f}")
        print(f"      - ì¤‘ê°„ê°’: {performance['subscriber_counts']['median']:,.0f}")
        
        print(f"   â±ï¸ ì˜ìƒ ê¸¸ì´:")
        print(f"      - í‰ê· : {performance['durations']['mean']/60:.1f}ë¶„")
        print(f"      - ì¤‘ê°„ê°’: {performance['durations']['median']/60:.1f}ë¶„")
        print(f"      - ìµœì¥: {performance['durations']['max']/60:.1f}ë¶„")
        print(f"      - ìµœë‹¨: {performance['durations']['min']/60:.1f}ë¶„")
        
        # 5. K-Beauty ê´€ë ¨ì„± ë¶„ì„
        kbeauty_analysis = self.analyze_kbeauty_relevance(data)
        print(f"\nğŸŒ¸ 5. K-Beauty ê´€ë ¨ì„± ë¶„ì„")
        print(f"   - K-Beauty ê´€ë ¨ ì˜ìƒ: {kbeauty_analysis['kbeauty_count']}ê°œ ({kbeauty_analysis['kbeauty_percentage']:.1f}%)")
        print(f"   - ì¼ë°˜ ì˜ìƒ: {kbeauty_analysis['normal_count']}ê°œ ({kbeauty_analysis['normal_percentage']:.1f}%)")
        
        print(f"\n   ğŸ”¥ ê°€ì¥ ë§ì´ ë°œê²¬ëœ K-Beauty í‚¤ì›Œë“œ:")
        for keyword, count in kbeauty_analysis['top_keywords']:
            print(f"      - {keyword}: {count}ê°œ")
        
        print(f"\n   ğŸ“‹ K-Beauty ê´€ë ¨ ì˜ìƒ ìƒ˜í”Œ (ì¡°íšŒìˆ˜ ìˆœ):")
        for i, sample in enumerate(kbeauty_analysis['top_samples'], 1):
            title = sample['title'][:60] + '...' if len(sample['title']) > 60 else sample['title']
            print(f"      {i}. {title}")
            print(f"         ì±„ë„: {sample['channel']} | ì¡°íšŒìˆ˜: {sample['views']:,}")
            if sample['keywords']:
                keywords = sample['keywords'][:50] + '...' if len(sample['keywords']) > 50 else sample['keywords']
                print(f"         í‚¤ì›Œë“œ: {keywords}")
        
        # 6. ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        quality_analysis = self.analyze_data_quality(data)
        print(f"\nğŸ” 6. ë°ì´í„° í’ˆì§ˆ ê²€ì¦")
        print(f"   - í•„ìˆ˜ ë°ì´í„° ëˆ„ë½: {quality_analysis['missing_data_count']}ê°œ ({quality_analysis['missing_data_percentage']:.1f}%)")
        print(f"   - ë°ì´í„° ì™„ì„±ë„: {quality_analysis['completeness_percentage']:.1f}%")
        
        # 7. ê³ ì„±ê³¼ ì˜ìƒ ë¶„ì„
        high_perf_analysis = self.analyze_high_performance(data)
        print(f"\nâ­ 7. ê³ ì„±ê³¼ ì˜ìƒ ë¶„ì„")
        print(f"   - ê³ ì„±ê³¼ ì˜ìƒ: {high_perf_analysis['high_performance_count']}ê°œ ({high_perf_analysis['high_performance_percentage']:.1f}%)")
        
        if high_perf_analysis['top_performers']:
            print(f"\n   ğŸ† ìƒìœ„ ê³ ì„±ê³¼ ì˜ìƒ:")
            for i, video in enumerate(high_perf_analysis['top_performers'], 1):
                title = video['title'][:50] + '...' if len(video['title']) > 50 else video['title']
                trending_mark = " ğŸ”¥" if video['is_trending'] else ""
                print(f"      {i}. {title}{trending_mark}")
                print(f"         ì¡°íšŒìˆ˜: {video['views']:,} | VPV: {video['vpv']:.2f} | ì°¸ì—¬ìœ¨: {video['engagement']*100:.2f}%")
                print(f"         ì±„ë„: {video['channel']}")
        
        # 8. ì±„ë„ ë¶„ì„
        channel_analysis = self.analyze_channels(data)
        print(f"\nğŸ“º 8. ì±„ë„ ë¶„ì„")
        print(f"   - ê³ ìœ  ì±„ë„ ìˆ˜: {channel_analysis['unique_channels']}ê°œ")
        print(f"   - ìƒìœ„ ì±„ë„ (ì˜ìƒ ìˆ˜ ê¸°ì¤€):")
        for channel, count in channel_analysis['top_channels']:
            print(f"      - {channel}: {count}ê°œ")
        
        # 9. AI í•™ìŠµ ì í•©ì„± í‰ê°€
        print(f"\nğŸ¤– 9. AI í•™ìŠµ ì í•©ì„± í‰ê°€")
        
        # íƒ€ê²Ÿ ë°¸ëŸ°ìŠ¤ í™•ì¸
        trending_ratio = trending_analysis['trending_percentage']
        if trending_ratio < 1:
            balance_score = "âš ï¸ ë¶ˆê· í˜• (íŠ¸ë Œë”© ì˜ìƒ ë¶€ì¡±)"
        elif trending_ratio > 10:
            balance_score = "âš ï¸ ë¶ˆê· í˜• (íŠ¸ë Œë”© ì˜ìƒ ê³¼ë‹¤)"
        else:
            balance_score = "âœ… ì–‘í˜¸"
        
        # ë°ì´í„° ë‹¤ì–‘ì„± í™•ì¸
        diversity_score = "âœ… ìš°ìˆ˜" if channel_analysis['unique_channels'] > 100 else "âš ï¸ ë³´í†µ"
        
        # K-Beauty ê´€ë ¨ì„± í™•ì¸
        relevance_score = "âœ… ìš°ìˆ˜" if kbeauty_analysis['kbeauty_percentage'] > 70 else "âš ï¸ ë³´í†µ"
        
        print(f"   - íƒ€ê²Ÿ ë°¸ëŸ°ìŠ¤: {balance_score} (íŠ¸ë Œë”© {trending_ratio:.1f}%)")
        print(f"   - ë°ì´í„° ë‹¤ì–‘ì„±: {diversity_score} (ì±„ë„ {channel_analysis['unique_channels']}ê°œ)")
        print(f"   - K-Beauty ê´€ë ¨ì„±: {relevance_score} ({kbeauty_analysis['kbeauty_percentage']:.1f}%)")
        print(f"   - ë°ì´í„° í’ˆì§ˆ: {'âœ… ìš°ìˆ˜' if quality_analysis['completeness_percentage'] > 95 else 'âš ï¸ ë³´í†µ'} ({quality_analysis['completeness_percentage']:.1f}%)")
        
        print(f"\n" + "=" * 80)
        print(f"ğŸ“‹ ë¶„ì„ ì™„ë£Œ! ì´ {basic_stats['total_records']:,}ê°œ ë ˆì½”ë“œ ë¶„ì„ë¨")


def find_latest_dataset(directory: str = "results") -> str:
    """ê°€ì¥ ìµœê·¼ ë°ì´í„°ì…‹ íŒŒì¼ ì°¾ê¸°"""
    pattern = os.path.join(directory, "youtube_viral_dataset_v1_*.csv")
    files = glob.glob(pattern)
    
    if not files:
        return None
    
    # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œí•˜ì—¬ ì •ë ¬
    files.sort(key=lambda x: os.path.basename(x).split('_')[-1].replace('.csv', ''), reverse=True)
    return files[0]


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="YouTube AI í•™ìŠµìš© ë°ì´í„°ì…‹ ë¶„ì„ê¸°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  %(prog)s                                    # ìµœì‹  íŒŒì¼ ìë™ ë¶„ì„
  %(prog)s --file dataset.csv                # íŠ¹ì • íŒŒì¼ ë¶„ì„
  %(prog)s --all                             # ëª¨ë“  íŒŒì¼ ë¶„ì„
        """
    )
    
    parser.add_argument(
        '--file', '-f',
        type=str,
        help='ë¶„ì„í•  CSV íŒŒì¼ ê²½ë¡œ'
    )
    
    parser.add_argument(
        '--all', '-a',
        action='store_true',
        help='results í´ë”ì˜ ëª¨ë“  ë°ì´í„°ì…‹ íŒŒì¼ ë¶„ì„'
    )
    
    parser.add_argument(
        '--directory', '-d',
        type=str,
        default='results',
        help='ë°ì´í„°ì…‹ íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: results)'
    )
    
    args = parser.parse_args()
    
    analyzer = YouTubeDatasetAnalyzer()
    
    if args.all:
        # ëª¨ë“  íŒŒì¼ ë¶„ì„
        pattern = os.path.join(args.directory, "youtube_viral_dataset_v1_*.csv")
        files = glob.glob(pattern)
        
        if not files:
            print(f"âŒ {args.directory} í´ë”ì—ì„œ ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        files.sort()
        print(f"ğŸ“ {len(files)}ê°œ íŒŒì¼ì„ ë¶„ì„í•©ë‹ˆë‹¤...\n")
        
        for i, file_path in enumerate(files, 1):
            print(f"\n{'='*20} íŒŒì¼ {i}/{len(files)} {'='*20}")
            analyzer.generate_report(file_path)
            
    elif args.file:
        # íŠ¹ì • íŒŒì¼ ë¶„ì„
        if not os.path.exists(args.file):
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.file}")
            return
        
        analyzer.generate_report(args.file)
        
    else:
        # ìµœì‹  íŒŒì¼ ìë™ ë¶„ì„
        latest_file = find_latest_dataset(args.directory)
        
        if not latest_file:
            print(f"âŒ {args.directory} í´ë”ì—ì„œ ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”:")
            print(f"python run_training_collection.py")
            return
        
        print(f"ğŸ“ ìµœì‹  íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì„ íƒí–ˆìŠµë‹ˆë‹¤: {os.path.basename(latest_file)}")
        analyzer.generate_report(latest_file)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìê°€ ë¶„ì„ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)